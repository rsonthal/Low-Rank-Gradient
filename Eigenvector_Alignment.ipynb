{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rsonthal/Low-Rank-Gradient/blob/main/Eigenvector_Alignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCmjoSs91cxD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import matplotlib.pyplot as plt # For optional plotting later\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8Nfkab_RDzA"
      },
      "outputs": [],
      "source": [
        "torch.set_default_device(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RKwZViXC1h2Z",
        "outputId": "23368b9e-01c5-49ba-9b7d-a8339f1545d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wuqGTdOH1dXO",
        "outputId": "bdf59a26-f6fe-4dd6-ca8d-44da356fc962"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using GPU: NVIDIA A100-SXM4-40GB\n"
          ]
        }
      ],
      "source": [
        "seed = 42\n",
        "\n",
        "# --- Device Setup ---\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"Using CPU\")\n",
        "\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "if device == torch.device(\"cuda\"):\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "# --- Activation Function ---\n",
        "relu_act = torch.relu\n",
        "sigmoid_act = torch.sigmoid\n",
        "tanh_act = torch.tanh\n",
        "\n",
        "def relative_projection_norm(W, X, y, q):\n",
        "  \"\"\"\n",
        "  Computes ||P_S(W)||_F / ||W||_F, where S = span(X^T*y, q).\n",
        "\n",
        "  Args:\n",
        "    W (torch.Tensor): Matrix to project (m, d).\n",
        "    X (torch.Tensor): Data matrix (n, d).\n",
        "    y (torch.Tensor): Target vector (n,) or (n, 1).\n",
        "    q (torch.Tensor): Second spanning vector (d,) or (d, 1).\n",
        "\n",
        "  Returns:\n",
        "    float: Relative Frobenius norm of the projection. Returns 0.0 if ||W||_F is zero.\n",
        "  \"\"\"\n",
        "  if y.ndim == 1: y = y.unsqueeze(1)\n",
        "  if q.ndim == 1: q = q.unsqueeze(1)\n",
        "  v1 = X.T @ y # Shape (d, 1)\n",
        "\n",
        "  # Create matrix of basis vectors (d, k), handle zero vectors implicitly via QR\n",
        "  # Concatenate non-zero vectors robustly\n",
        "  vec_list = [v for v in [v1, q] if torch.linalg.norm(v) > 1e-9]\n",
        "  if not vec_list: return 0.0\n",
        "  basis_mat = torch.cat(vec_list, dim=1)\n",
        "\n",
        "  # Orthonormal basis Q (d, k) for the span (k=rank, <=2)\n",
        "  Q, _ = torch.linalg.qr(basis_mat, mode='reduced')\n",
        "\n",
        "  # Norm of projection || P_S(W) ||_F = || W @ Q ||_F\n",
        "  norm_proj = torch.linalg.norm(W @ Q)\n",
        "  norm_W = torch.linalg.norm(W)\n",
        "\n",
        "  return (norm_proj / norm_W).item() if norm_W > 1e-9 else 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3mK_0UM5dbYl"
      },
      "outputs": [],
      "source": [
        "def operator_norm(A):\n",
        "  return largest_singular_value(A)\n",
        "\n",
        "def largest_singular_value(A, num_iterations=200):\n",
        "    \"\"\"\n",
        "    Approximates the largest singular value of a matrix using power iteration.\n",
        "\n",
        "    Args:\n",
        "        A: The input matrix (torch.Tensor).\n",
        "        num_iterations: The number of iterations to perform.\n",
        "\n",
        "    Returns:\n",
        "        The estimated largest singular value.\n",
        "    \"\"\"\n",
        "\n",
        "    v = torch.randn(A.shape[1], device=A.device)  # Random initialization\n",
        "    v = v / torch.norm(v)\n",
        "\n",
        "    for _ in range(num_iterations):\n",
        "        u = A @ v\n",
        "        v = A.T @ u\n",
        "        v = v / torch.norm(v)\n",
        "\n",
        "    return torch.norm(u)  # Largest singular value approximation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ZdcVrORmWXV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def principal_angles(A: torch.Tensor,\n",
        "                     B: torch.Tensor,\n",
        "                     *,\n",
        "                     rtol=1e-12,\n",
        "                     atol=1e-14):\n",
        "    \"\"\"\n",
        "    Principal angles (radians) between the column spaces of A and B.\n",
        "    Returns the angles in *ascending* order.\n",
        "\n",
        "    The routine\n",
        "      • works with real or complex tensors\n",
        "      • keeps only the numerically significant singular vectors\n",
        "      • uses SVD (more stable than unpivoted QR)\n",
        "    \"\"\"\n",
        "\n",
        "    if A.shape[0] != B.shape[0]:\n",
        "        raise ValueError(\"A and B must have the same number of rows\")\n",
        "\n",
        "    # promote to float64 unless the user already did so\n",
        "    if A.dtype == torch.float32:\n",
        "        A = A.double()\n",
        "        B = B.double()\n",
        "\n",
        "    # --- orthonormal bases --------------------------------------------------\n",
        "    #\n",
        "    #   A = U_A Σ_A V_A^H   (thin SVD)\n",
        "    #   keep columns of U_A corresponding to Σ_A > τ\n",
        "    #\n",
        "    def orthonormal_basis(X):\n",
        "        U, S, _ = torch.linalg.svd(X, full_matrices=False)\n",
        "        τ = atol + rtol * S.max()\n",
        "        r = (S > τ).sum().item()          # numerical rank\n",
        "        return U[:, :r]                   # m × r  (possibly r = 0)\n",
        "\n",
        "    UA, UB = map(orthonormal_basis, (A, B))\n",
        "    if UA.shape[1] == 0 or UB.shape[1] == 0:\n",
        "        # One of the spaces is {0}; caller can decide what to do\n",
        "        return torch.empty(0, dtype=A.dtype, device=A.device)\n",
        "\n",
        "    # --- cosines of principal angles ---------------------------------------\n",
        "    C = UA.conj().T @ UB                 # r_A × r_B\n",
        "    σ = torch.linalg.svdvals(C)          # singular values, descending\n",
        "    σ = torch.clamp(σ, 0.0, 1.0)         # safety against roundoff\n",
        "\n",
        "    angles = torch.acos(σ)\n",
        "    angles, _ = torch.sort(angles)       # explicit ascending order\n",
        "    return angles\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QKapKxrPxKwe"
      },
      "outputs": [],
      "source": [
        "def activation(x, act = \"relu\"):\n",
        "  if act == \"relu\":\n",
        "    # print(\"Activation ReLU\")\n",
        "    return torch.relu(x)\n",
        "  elif act == \"sigmoid\":\n",
        "    # print(\"Activation Sigmoid\")\n",
        "    return torch.sigmoid(x)\n",
        "  elif act == \"tanh\":\n",
        "    # print(\"Activation Tanh\")\n",
        "    return torch.tanh(x)\n",
        "\n",
        "def activation_derivative(x, act = \"relu\"):\n",
        "  if act == \"relu\":\n",
        "    # print(\"Activation ReLU Derivative\")\n",
        "    return (x > 0).float()\n",
        "  elif act == \"sigmoid\":\n",
        "    # print(\"Activation Sigmoid Derivative\")\n",
        "    z = activation(x, act = act)\n",
        "    return z * (1-z)\n",
        "  elif act == \"tanh\":\n",
        "    # print(\"Activation Tanh Derivative\")\n",
        "    z = activation(x, act = act)\n",
        "    return 1 - z**2\n",
        "\n",
        "def gen_data(n, d, zeta, q, alpha = 0, return_all = False):\n",
        "  \"\"\"\n",
        "    - q should be a d x 1 vector\n",
        "  \"\"\"\n",
        "  Sigma = torch.diag(torch.arange(1,d+1) ** (-1*alpha/2))\n",
        "  X_B = torch.randn(n,d) @ Sigma\n",
        "  z = torch.randn(n,1)\n",
        "  if not return_all:\n",
        "    return X_B + zeta * z @ q.T\n",
        "  else:\n",
        "    return X_B + zeta * z @ q.T, z, X_B, zeta * z @ q.T\n",
        "\n",
        "def get_y(X, f, tau = 1, loss_type = \"MSE\"):\n",
        "  if loss_type == \"MSE\":\n",
        "    y = torch.tensor([f(X[[i],:].T) for i in range(X.shape[0])])\n",
        "    return y + torch.randn(y.shape) * tau\n",
        "  elif loss_type == \"bce\":\n",
        "    return torch.tensor([f(X[[i],:].T) for i in range(X.shape[0])])\n",
        "  elif loss_type == \"hinge\":\n",
        "    y = torch.tensor([f(X[[i],:].T) for i in range(X.shape[0])])\n",
        "    return (y-0.5).sign()\n",
        "  y = torch.tensor([f(X[[i],:].T) for i in range(X.shape[0])])\n",
        "  return y + torch.randn(y.shape) * tau\n",
        "\n",
        "def loss(X, W, a, y, gamma, act = \"relu\", loss_type = \"MSE\"):\n",
        "  F = gamma * a.T @ activation(W @ X.T, act = act)\n",
        "  if loss_type == \"MSE\":\n",
        "    return torch.mean((F - y)**2)/2\n",
        "  elif loss_type == \"bce\":\n",
        "    z = activation(F, act = \"sigmoid\")\n",
        "    return torch.mean(-y * torch.log(z) - (1-y) * torch.log(1-z))\n",
        "  elif loss_type == \"hinge\":\n",
        "    return torch.mean(torch.relu(1 - y * F))\n",
        "\n",
        "def loss_derivative(X, W, a, y, gamma, act = \"relu\", loss_type = \"MSE\"):\n",
        "  F = gamma * a.T @ activation(W @ X.T, act = act)\n",
        "  if loss_type == \"MSE\":\n",
        "    # print(\"Here\")\n",
        "    # print(F.shape, y.shape)\n",
        "    return (F - y)\n",
        "  elif loss_type == \"bce\":\n",
        "    z = activation(F, act = \"sigmoid\")\n",
        "    return z - y\n",
        "  elif loss_type == \"hinge\":\n",
        "    return torch.where(y * F < 1, -y, 0)\n",
        "\n",
        "def get_mu(zeta, q, W, alpha, gamma, act = \"sigmoid\", loss_type = \"MSE\", n = 10000):\n",
        "  d = q.shape[0]\n",
        "  X = gen_data(n, d, zeta, q, alpha)\n",
        "  Z = activation_derivative(W @ X.T, act = act)\n",
        "  return Z.mean(dim = 1)\n",
        "\n",
        "def activation_derivative_perp(X, W, mu, act = \"relu\"):\n",
        "  return activation_derivative(X @ W.T, act = act) - mu.view(1,-1)\n",
        "\n",
        "def activation_derivative_S2(X, W, mu, act = \"relu\"):\n",
        "  return activation_derivative(X @ W.T, act = act)\n",
        "\n",
        "def get_S1_exp(zeta, q, f, a, mu, alpha, gamma,  act = \"relu\", loss_type = \"MSE\", n = 10000):\n",
        "  d = q.shape[0]\n",
        "  X = gen_data(n, d, zeta, q, alpha)\n",
        "  y = get_y(X, f)\n",
        "  r = loss_derivative(X, W, a, y, gamma, act = act, loss_type = loss_type).view(-1,1)\n",
        "  return (X.T @ r @ (a * mu).T) / n\n",
        "\n",
        "def get_S1(X, r, a, mu):\n",
        "  # print(mu.shape, a.shape)\n",
        "  return X.T @ r @ (a * mu).T / X.shape[0]\n",
        "\n",
        "def get_S1_perp(X, W, mu, a, r, act = \"relu\", loss_type = \"MSE\"):\n",
        "  sigma_prime_perp = activation_derivative_perp(X, W, mu, act = act)\n",
        "  return X.T @ ((r @ a.T) * sigma_prime_perp) / X.shape[0]\n",
        "\n",
        "def get_G(X, W, y, r, a, act = \"relu\", loss_type = \"MSE\"):\n",
        "  sigma_prime = activation_derivative(X @ W.T, act = act)\n",
        "\n",
        "  G = X.T @ ((r @ a.T) * sigma_prime)\n",
        "  return G / X.shape[0]\n",
        "\n",
        "def get_S2(X, X_S, mu, r, W, a, y, alpha, act = \"relu\", loss_type = \"MSE\"):\n",
        "  d = q.shape[0]\n",
        "  sigma_prime_perp = activation_derivative_S2(X, W, mu, act = act)\n",
        "\n",
        "  return X_S.T @ ((r @ a.T) * sigma_prime_perp) / X_S.shape[0]\n",
        "\n",
        "def get_S2_small(X, X_S, mu, r, W, a, y, alpha, act = \"relu\", loss_type = \"MSE\"):\n",
        "  d = q.shape[0]\n",
        "  sigma_prime_perp = activation_derivative_perp(X, W, mu, act = act)\n",
        "\n",
        "  return X_S.T @ ((r @ a.T) * sigma_prime_perp) / X_S.shape[0]\n",
        "\n",
        "def get_E(X, X_B, W, mu, a, r, y, act = \"relu\", loss_type = \"MSE\"):\n",
        "  sigma_prime_perp = activation_derivative_perp(X, W, mu, act = act)\n",
        "\n",
        "  return X_B.T @ ((r @ a.T) * sigma_prime_perp) / X_B.shape[0]\n",
        "\n",
        "def operator_norm(A):\n",
        "  return largest_singular_value(A)\n",
        "\n",
        "def largest_singular_value(A, num_iterations=200):\n",
        "    \"\"\"\n",
        "    Approximates the largest singular value of a matrix using power iteration.\n",
        "\n",
        "    Args:\n",
        "        A: The input matrix (torch.Tensor).\n",
        "        num_iterations: The number of iterations to perform.\n",
        "\n",
        "    Returns:\n",
        "        The estimated largest singular value.\n",
        "    \"\"\"\n",
        "\n",
        "    v = torch.randn(A.shape[1], device=A.device)  # Random initialization\n",
        "    v = v / torch.norm(v)\n",
        "\n",
        "    for _ in range(num_iterations):\n",
        "        u = A @ v\n",
        "        v = A.T @ u\n",
        "        v = v / torch.norm(v)\n",
        "\n",
        "    return torch.norm(u)  # Largest singular value approximation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68l3y2184QLT"
      },
      "outputs": [],
      "source": [
        "# --- Define the Network Function f(X) ---\n",
        "def network_forward(X_input, W_layer, a_vec, activation_fn, gamma):\n",
        "    \"\"\"\n",
        "    Computes the forward pass of the network y = a^T * sigma(W @ X^T).\n",
        "\n",
        "    Args:\n",
        "        X_input (torch.Tensor): Input data shape (N, d).\n",
        "        W_layer (torch.Tensor): Inner weight matrix shape (k, d).\n",
        "        a_vec (torch.Tensor): Outer fixed weight vector shape (k, 1).\n",
        "        activation_fn (callable): Activation function (e.g., torch.relu).\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Network output prediction shape (N, 1).\n",
        "    \"\"\"\n",
        "    # Z = W @ X.T  -> shape (k, N)\n",
        "    Z = W_layer @ X_input.T\n",
        "    # H = sigma(Z) -> shape (k, N)\n",
        "    H = activation_fn(Z)\n",
        "    # y_pred_transposed = a.T @ H -> shape (1, N)\n",
        "    y_pred_transposed = gamma * a_vec.T @ H\n",
        "    # y_pred -> shape (N, 1)\n",
        "    y_pred = y_pred_transposed.T\n",
        "    return gamma*y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HvXysM4clT6x"
      },
      "outputs": [],
      "source": [
        "def train_track(X,y,Xtst,ytst,W,a,act,gamma,lr,epochs):\n",
        "  losses = []\n",
        "  operator_norms = []\n",
        "  spectrum = []\n",
        "  leading_right_svs = [] # List to store the vectors themselves\n",
        "  Ws = []\n",
        "  test_losses = []\n",
        "  energy = []\n",
        "\n",
        "  print(f\"Starting training for {epochs} epochs with lr={lr}\")\n",
        "  for epoch in range(epochs):\n",
        "\n",
        "      # --- Forward Pass using the defined function f(X) ---\n",
        "      y_pred = network_forward(X, W, a, act, gamma = gamma)\n",
        "      loss = torch.mean((y_pred - y)**2)\n",
        "\n",
        "      if W.grad is not None:\n",
        "          W.grad.zero_()\n",
        "\n",
        "      loss.backward() # Computes dL/dW\n",
        "\n",
        "      with torch.no_grad(): # Context manager to disable gradient tracking for the update\n",
        "          # Gradient Descent step\n",
        "          W -= lr/gamma * W.grad\n",
        "\n",
        "          # Renormalize rows of W to have unit norm\n",
        "          W_norms = torch.norm(W, p=2, dim=1, keepdim=True)\n",
        "          W /= (W_norms + 1e-8)\n",
        "\n",
        "          U, S, Vh = torch.linalg.svd(W.grad, full_matrices=False)\n",
        "\n",
        "          # Operator norm is the largest singular value\n",
        "          op_norm = operator_norm(W) #S[0].item() # S is sorted in descending order\n",
        "          operator_norms.append(op_norm.detach().cpu())\n",
        "\n",
        "          spectrum.append(S.clone().detach().cpu())\n",
        "\n",
        "          Ws.append(W.clone().detach().cpu())\n",
        "\n",
        "          # Leading right singular vector is the first row of Vh\n",
        "          lead_right_sv = Vh[0, :].clone() # Store a copy!\n",
        "          leading_right_svs.append(lead_right_sv.detach().cpu())\n",
        "\n",
        "          test_loss = torch.mean((network_forward(Xtst, W, a, act, gamma = gamma) - ytst)**2)\n",
        "          test_losses.append(test_loss.detach().cpu().item())\n",
        "\n",
        "\n",
        "      # Store loss for plotting/analysis\n",
        "      losses.append(loss.detach().cpu().item())\n",
        "      energy.append(relative_projection_norm(Ws[-1].to('cuda'),X_B,y,q))\n",
        "\n",
        "      # --- Clear Gradients ---\n",
        "      W.grad.zero_()\n",
        "\n",
        "      # Print progress\n",
        "      if (epoch + 1) % 50 == 0:\n",
        "          print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.6f}, Test Loss: {test_loss.item():.6f}, Operator Norm: {operator_norms[-1]:.4f}\")\n",
        "\n",
        "  print(\"Training finished.\")\n",
        "\n",
        "  a = torch.linalg.pinv(act(X @ W.T)) @ y\n",
        "\n",
        "  y_test = network_forward(Xtst, W, a, act, gamma = gamma)\n",
        "  test_loss = torch.mean((y_test - ytst)**2)\n",
        "  print(f\"Final Test Loss: {test_loss.item():.6f}\")\n",
        "\n",
        "  return losses, operator_norms, spectrum, leading_right_svs, Ws, test_losses, energy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vYbq8E4tm_QT"
      },
      "outputs": [],
      "source": [
        "def plot_train_test_loss(losses, test_losses, label):\n",
        "  fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "  # Plot Loss\n",
        "  axs[0].plot(range(epochs), losses)\n",
        "  axs[0].set_xlabel(\"Epoch\", fontsize = 16)\n",
        "  axs[0].set_ylabel(\"Train MSE Loss\", fontsize = 16)\n",
        "  axs[0].set_title(label, fontsize = 16)\n",
        "  axs[0].grid(True)\n",
        "  # axs[0].set_yscale('log') # Log scale often helpful for loss\n",
        "\n",
        "  # Plot Operator Norm\n",
        "  axs[1].plot(range(epochs), test_losses)\n",
        "  axs[1].set_xlabel(\"Epoch\", fontsize = 16)\n",
        "  axs[1].set_ylabel(\"Test MSE Loss\", fontsize = 16)\n",
        "  axs[1].set_title(label, fontsize = 16)\n",
        "  axs[1].grid(True)\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "  plt.savefig(path+f\"Loss-{label}-gamma-{gamma}-lr-{lr}.png\", dpi = 100)\n",
        "\n",
        "\n",
        "  plt.close()\n",
        "\n",
        "def plot_norms(spectrum, operator_norms, label):\n",
        "  fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "  S = torch.zeros(len(spectrum))\n",
        "  for i in range(len(spectrum)):\n",
        "    S[i] = spectrum[i].max().item()\n",
        "\n",
        "  # Plot Loss\n",
        "  axs[0].plot(range(epochs), S.cpu())\n",
        "  axs[0].set_xlabel(\"Epoch\", fontsize = 16)\n",
        "  axs[0].set_ylabel(\"Gradient G Operator Norm\", fontsize = 16)\n",
        "  axs[0].set_title(label, fontsize = 16)\n",
        "  axs[0].grid(True)\n",
        "  # axs[0].set_yscale('log') # Log scale often helpful for loss\n",
        "\n",
        "  # Plot Operator Norm\n",
        "  axs[1].plot(range(epochs), operator_norms)\n",
        "  axs[1].set_xlabel(\"Epoch\", fontsize = 16)\n",
        "  axs[1].set_ylabel(\"Weight W operator norm\", fontsize = 16)\n",
        "  axs[1].set_title(label, fontsize = 16)\n",
        "  axs[1].grid(True)\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "  plt.savefig(path+f\"Norms-{label}-gamma-{gamma}-lr-{lr}.png\", dpi = 100)\n",
        "\n",
        "\n",
        "  plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8skDX7Yapexa"
      },
      "outputs": [],
      "source": [
        "def alignment_plot(leading_right_svs, label, epochs, q = None, Ws = None, a = None, act = None, gamma = None):\n",
        "  # You can further analyze the evolution of the leading_right_svs list\n",
        "  # For example, calculate the cosine similarity between consecutive vectors\n",
        "  similarities = []\n",
        "  for i in range(len(leading_right_svs)):\n",
        "      # Ensure vectors are on CPU for numpy if needed, or use torch dot product\n",
        "      vec1 = leading_right_svs[i]\n",
        "      vec1 /= vec1.norm()\n",
        "      if q is not None:\n",
        "        vec2 = q.flatten()\n",
        "      else:\n",
        "        vec2 = (X_B.T.cpu() @ (y.cpu() - network_forward(X.cpu(), Ws[i].cpu(), a.cpu(), act, gamma = gamma)) ).flatten() #\n",
        "      vec2 /= vec2.norm() #leading_right_svs[i+1]\n",
        "      # Cosine similarity = dot(v1, v2) / (norm(v1) * norm(v2))\n",
        "      # Since norms should be 1, it's just the dot product\n",
        "      sim = torch.dot(vec1.cpu(), vec2.cpu()).abs().item()\n",
        "      similarities.append(sim)\n",
        "\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  plt.plot(range(epochs), similarities)\n",
        "  plt.xlabel(\"Epoch\", fontsize = 16)\n",
        "  plt.ylabel(r\"Dot Product\", fontsize = 16)\n",
        "  plt.title(label, fontsize = 16)\n",
        "  plt.grid(True)\n",
        "  plt.xscale('log')\n",
        "  # plt.show()\n",
        "\n",
        "  plt.savefig(path+f\"Alignment-{label}-gamma-{gamma}-lr-{lr}.png\", dpi = 100)\n",
        "\n",
        "  plt.close()\n",
        "\n",
        "def angles_plot(Ws1, Ws2, label, gamma = None):\n",
        "  angles_deg = torch.zeros(epochs)\n",
        "\n",
        "  for i in tqdm(range(epochs)):\n",
        "      angles_rad = principal_angles(Ws1[i], Ws2[i])\n",
        "      angles_deg[i] = torch.rad2deg(angles_rad).mean()\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  plt.scatter(range(epochs), angles_deg.cpu())\n",
        "  plt.xlabel(\"Epoch\", fontsize = 16)\n",
        "  plt.ylabel(r\"Mean Prinicipal Angle\", fontsize = 16)\n",
        "  plt.title(label, fontsize = 16)\n",
        "  plt.grid(True)\n",
        "  # plt.xscale('log')\n",
        "  # plt.show()\n",
        "\n",
        "  plt.savefig(path+f\"Angle-{label}-gamma-{gamma}-lr-{lr}.png\", dpi = 100)\n",
        "\n",
        "\n",
        "  plt.close()\n",
        "\n",
        "def energy_plot(energy, label, gamma = None):\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  plt.scatter(range(epochs), energy)\n",
        "  plt.xlabel(\"Epoch\", fontsize = 16)\n",
        "  plt.ylabel(r\"Proportion in Rank 2 Subspace\", fontsize = 16)\n",
        "  plt.title(label, fontsize = 16)\n",
        "  plt.grid(True)\n",
        "  # plt.xscale('log')\n",
        "  # plt.show()\n",
        "\n",
        "  plt.savefig(path+f\"Energy-{label}-gamma-{gamma}-lr-{lr}.png\", dpi = 100)\n",
        "\n",
        "\n",
        "  plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krq_dz4NxKpV"
      },
      "source": [
        "# ReLU vs Sigmoid Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CscjCEsI6d8B"
      },
      "outputs": [],
      "source": [
        "def run_exp_and_plot(lr,gamma,W,a):\n",
        "\n",
        "  W_relu = copy.deepcopy(W)\n",
        "  a_relu = copy.deepcopy(a)\n",
        "\n",
        "  W_sigmoid = copy.deepcopy(W)\n",
        "  a_sigmoid = copy.deepcopy(a)\n",
        "  losses_relu, operator_norms_relu, spectrum_relu, leading_right_svs_relu, Ws_relu, test_losses_relu, energy_relu = train_track(X,y,Xtst,ytst,W_relu,a_relu,relu_act,gamma,lr,epochs)\n",
        "  losses_sigmoid, operator_norms_sigmoid, spectrum_sigmoid, leading_right_svs_sigmoid, Ws_sigmoid, test_losses_sigmoid, energy_sigmoid = train_track(X,y,Xtst,ytst,W_sigmoid,a_sigmoid,sigmoid_act,gamma,lr,epochs)\n",
        "  plot_train_test_loss(losses_relu, test_losses_relu, \"ReLU\")\n",
        "  plot_norms(spectrum_relu, operator_norms_relu, \"ReLU\")\n",
        "  plot_train_test_loss(losses_sigmoid, test_losses_sigmoid, \"Sigmoid\")\n",
        "  plot_norms(spectrum_sigmoid, operator_norms_sigmoid, \"Sigmoid\")\n",
        "  alignment_plot(leading_right_svs_relu, \"ReLU Residue Alignment\", epochs, Ws = Ws_relu, a = a_relu, act = relu_act, gamma = gamma)\n",
        "  alignment_plot(leading_right_svs_relu, \"ReLU Data Alignment\", epochs, q = q)\n",
        "  alignment_plot(leading_right_svs_sigmoid, \"Sigmoid Residue Alignment\", epochs, Ws = Ws_sigmoid, a = a_sigmoid, act = sigmoid_act, gamma = gamma)\n",
        "  alignment_plot(leading_right_svs_sigmoid, \"Sigmoid Data Alignment\", epochs, q = q)\n",
        "\n",
        "  energy_plot(energy_relu, \"ReLU\")\n",
        "  energy_plot(energy_sigmoid, \"Sigmoid\")\n",
        "\n",
        "  # angles_plot(Ws_relu, Ws_sigmoid, \"ReLU vs Sigmoid\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oEMp2mAzVhbG"
      },
      "outputs": [],
      "source": [
        "def run_exp_and_plot_regression(lr,gamma,W,a):\n",
        "\n",
        "  W_relu = copy.deepcopy(W)\n",
        "  a_relu = copy.deepcopy(a)\n",
        "\n",
        "  W_sigmoid = copy.deepcopy(W)\n",
        "  a_sigmoid = copy.deepcopy(a)\n",
        "\n",
        "  if gamma > 1/2:\n",
        "    print(\"NTK Regime \\n\")\n",
        "  else:\n",
        "    print(\"MF Regime \\n\")\n",
        "\n",
        "\n",
        "  print(\"ReLU\")\n",
        "  losses_relu, operator_norms_relu, spectrum_relu, leading_right_svs_relu, Ws_relu, test_losses_relu = train_track(X,y,Xtst,ytst,W_relu,a_relu,relu_act,gamma,lr,epochs)\n",
        "\n",
        "  r = (X_B.T.cpu() @ (y.cpu()- network_forward(X.cpu(), Ws_relu[-1].cpu(), a.cpu(), relu_act, gamma = gamma) ) )\n",
        "\n",
        "  print(\"Residue Alignment\", leading_right_svs_relu[-1].view(1,-1) @ (r) / (r.norm()))\n",
        "  print(\"Data Alignment\", leading_right_svs_relu[-1].view(1,-1) @ (q), \"\\n\")\n",
        "\n",
        "  print(\"Sigmoid\")\n",
        "\n",
        "  losses_sigmoid, operator_norms_sigmoid, spectrum_sigmoid, leading_right_svs_sigmoid, Ws_sigmoid, test_losses_sigmoid = train_track(X,y,Xtst,ytst,W_sigmoid,a_sigmoid,sigmoid_act,gamma,lr,epochs)\n",
        "\n",
        "  r = (X_B.T.cpu() @ (y.cpu()- network_forward(X.cpu(), Ws_sigmoid[-1].cpu(), a.cpu(), sigmoid_act, gamma = gamma) ) )\n",
        "\n",
        "  print(\"Residue Alignment\", leading_right_svs_sigmoid[-1].view(1,-1) @ (r) / (r.norm()))\n",
        "  print(\"Data Alignment\", leading_right_svs_sigmoid[-1].view(1,-1) @ (q), \"\\n\")\n",
        "\n",
        "  # plot_train_test_loss(losses_relu, test_losses_relu, \"ReLU\")\n",
        "  # plot_norms(spectrum_relu, operator_norms_relu, \"ReLU\")\n",
        "  # plot_train_test_loss(losses_sigmoid, test_losses_sigmoid, \"Sigmoid\")\n",
        "  # plot_norms(spectrum_sigmoid, operator_norms_sigmoid, \"Sigmoid\")\n",
        "  # alignment_plot(leading_right_svs_relu, \"ReLU Residue Alignment\", epochs, Ws = Ws_relu, a = a_relu, act = relu_act, gamma = gamma)\n",
        "  # alignment_plot(leading_right_svs_relu, \"ReLU Data Alignment\", epochs, q = q)\n",
        "  # alignment_plot(leading_right_svs_sigmoid, \"Sigmoid Residue Alignment\", epochs, Ws = Ws_sigmoid, a = a_sigmoid, act = sigmoid_act, gamma = gamma)\n",
        "  # alignment_plot(leading_right_svs_sigmoid, \"Sigmoid Data Alignment\", epochs, q = q)\n",
        "  # angles_plot(Ws_relu, Ws_sigmoid, \"ReLU vs Sigmoid\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X5QFLF-xxri1"
      },
      "outputs": [],
      "source": [
        "nu = 1/8\n",
        "alpha = 0\n",
        "\n",
        "# --- Configuration ---\n",
        "seed = 42          # For reproducibility\n",
        "epochs = 100       # Number of training iterations\n",
        "d = 1000            # Input dimension\n",
        "k = 1250           # Number of hidden neurons (rows of W)\n",
        "N = 750            # Number of data points (full batch)\n",
        "\n",
        "q = torch.randn(d,1)\n",
        "q = q/torch.norm(q)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJdudE-h1dfm",
        "outputId": "312da362-ac8f-45a4-f67d-48f15f9705d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing model: k=1250, d=1000\n",
            "Model initialized.\n"
          ]
        }
      ],
      "source": [
        "# --- Model Initialization ---\n",
        "print(f\"Initializing model: k={k}, d={d}\")\n",
        "# Initialize W with rows uniformly on the unit sphere\n",
        "W = torch.randn(k, d, device=device)\n",
        "W = W/ torch.norm(W, p=2, dim=1, keepdim=True)\n",
        "W.requires_grad_(True) # We want to compute gradients for W\n",
        "\n",
        "# Initialize fixed vector 'a' with Gaussian entries (scaled)\n",
        "# Variance 1/k ensures E[||a||^2] = 1\n",
        "a = (torch.randint(0, 2, (k,1), device = device).float() - 1)/np.sqrt(k)\n",
        "a.requires_grad_(False) # 'a' is fixed\n",
        "print(\"Model initialized.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8TO9FkbxnJE"
      },
      "outputs": [],
      "source": [
        "beta = torch.randn(d,1)\n",
        "beta = beta/torch.norm(beta)\n",
        "\n",
        "def f(x):\n",
        "  return activation(x.T @ beta, act = \"sigmoid\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NCYnQDx8xuGp"
      },
      "outputs": [],
      "source": [
        "X, z, X_B, X_S = gen_data(N, d, N**nu, q, alpha, return_all=True)\n",
        "y = get_y(X, f).reshape(-1,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "drJJePbsySqP"
      },
      "outputs": [],
      "source": [
        "X = X.to('cuda')\n",
        "y = y.to('cuda')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oTsb11Sthwuc"
      },
      "outputs": [],
      "source": [
        "Xtst = gen_data(N, d, N**nu, q, alpha)\n",
        "ytst = get_y(Xtst, f)\n",
        "\n",
        "Xtst = Xtst.to('cuda')\n",
        "ytst = ytst.to('cuda')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-pY4FbF7J-x"
      },
      "outputs": [],
      "source": [
        "lr = 2*np.sqrt(k)             # Fixed learning rate\n",
        "gamma = 1/np.sqrt(k)\n",
        "path = \"\"\n",
        "run_exp_and_plot(lr,gamma,copy.deepcopy(W),copy.deepcopy(a))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SgtUzLf97j5p"
      },
      "outputs": [],
      "source": [
        "lr = 2*np.sqrt(k)             # Fixed learning rate\n",
        "gamma = 1\n",
        "path = \"drive/MyDrive/Spikes/ReLU VS Sigmoid/\"\n",
        "run_exp_and_plot(lr,gamma,copy.deepcopy(W),copy.deepcopy(a))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cfpremFN7nSS"
      },
      "outputs": [],
      "source": [
        "lr = 2         # Fixed learning rate\n",
        "gamma = 1/np.sqrt(k)\n",
        "path = \"drive/MyDrive/Spikes/ReLU VS Sigmoid/Exp9/\"\n",
        "run_exp_and_plot(lr,gamma,copy.deepcopy(W),copy.deepcopy(a))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1Ghv4RpH7sI9"
      },
      "outputs": [],
      "source": [
        "lr = 2 * np.sqrt(k)            # Fixed learning rate\n",
        "epochs = 1000\n",
        "gamma = 1/k\n",
        "path = \"drive/MyDrive/Spikes/ReLU VS Sigmoid/Exp11/\"\n",
        "run_exp_and_plot(lr,gamma,copy.deepcopy(W),copy.deepcopy(a))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eM-Fgyln75N9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VoKoaX-g75Th"
      },
      "outputs": [],
      "source": [
        "beta = torch.randn(d,1)\n",
        "beta = beta/torch.norm(beta)\n",
        "\n",
        "def f(x):\n",
        "  return activation(x.T @ beta, act = \"relu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WtpwVMDy75Ti"
      },
      "outputs": [],
      "source": [
        "X, z, X_B, X_S = gen_data(N, d, N**nu, q, alpha, return_all=True)\n",
        "y = get_y(X, f).reshape(-1,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HS4uoIku75Ti"
      },
      "outputs": [],
      "source": [
        "X = X.to('cuda')\n",
        "y = y.to('cuda')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MYvn5nBn75Ti"
      },
      "outputs": [],
      "source": [
        "Xtst = gen_data(N, d, N**nu, q, alpha)\n",
        "ytst = get_y(Xtst, f)\n",
        "\n",
        "Xtst = Xtst.to('cuda')\n",
        "ytst = ytst.to('cuda')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_reFYDD75Ti"
      },
      "outputs": [],
      "source": [
        "lr = 2             # Fixed learning rate\n",
        "gamma = 1/np.sqrt(k)\n",
        "path = \"drive/MyDrive/Spikes/ReLU VS Sigmoid/Exp2/\"\n",
        "run_exp_and_plot(lr,gamma,copy.deepcopy(W),copy.deepcopy(a))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QG5-6zBY75Ti"
      },
      "outputs": [],
      "source": [
        "lr = 2             # Fixed learning rate\n",
        "gamma = 1\n",
        "path = \"drive/MyDrive/Spikes/ReLU VS Sigmoid/Exp4/\"\n",
        "run_exp_and_plot(lr,gamma,copy.deepcopy(W),copy.deepcopy(a))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QLUzNdMO75Ti"
      },
      "outputs": [],
      "source": [
        "lr = 2*np.sqrt(k)             # Fixed learning rate\n",
        "gamma = 1/np.sqrt(k)\n",
        "path = \"drive/MyDrive/Spikes/ReLU VS Sigmoid/Exp10/\"\n",
        "run_exp_and_plot(lr,gamma,copy.deepcopy(W),copy.deepcopy(a))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oX8Goyd_75Ti"
      },
      "outputs": [],
      "source": [
        "lr = 2*np.sqrt(k)             # Fixed learning rate\n",
        "gamma = 1\n",
        "path = \"drive/MyDrive/Spikes/ReLU VS Sigmoid/Exp12/\"\n",
        "run_exp_and_plot(lr,gamma,copy.deepcopy(W),copy.deepcopy(a))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MvZq0KHxa1z"
      },
      "source": [
        "# NTK vs Meanfield"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdvLzENwxavT"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "def alignment_plot_initial(X, y, a, leading_right_svs, sigma, label, epochs,gamma = None):\n",
        "  # You can further analyze the evolution of the leading_right_svs list\n",
        "  # For example, calculate the cosine similarity between consecutive vectors\n",
        "  similarities = []\n",
        "  X = X.cpu()\n",
        "  a = a.cpu()\n",
        "  y = y.cpu()\n",
        "  for i in range(len(leading_right_svs)):\n",
        "      # Ensure vectors are on CPU for numpy if needed, or use torch dot product\n",
        "      vec1 = leading_right_svs[0]\n",
        "      vec1 /= vec1.norm() #leading_right_svs[i]\n",
        "      vec2 = leading_right_svs[i]\n",
        "      vec2 /= vec2.norm()\n",
        "\n",
        "      # Cosine similarity = dot(v1, v2) / (norm(v1) * norm(v2))\n",
        "      # Since norms should be 1, it's just the dot product\n",
        "      sim = torch.dot(vec1.cpu(), vec2.cpu()).abs().item()\n",
        "      similarities.append(sim)\n",
        "\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  plt.scatter(range(epochs), similarities)\n",
        "  plt.xlabel(\"Epoch\", fontsize = 16)\n",
        "  plt.ylabel(r\"Dot Product\", fontsize = 16)\n",
        "  # plt.title(label, fontsize = 16)\n",
        "  plt.grid(True)\n",
        "  # plt.xscale('log')\n",
        "  # plt.show()\n",
        "\n",
        "  plt.savefig(path+f\"Alignment-initial-{label}-gamma-{gamma}-lr-{lr}.png\", dpi = 100)\n",
        "\n",
        "  plt.close()\n",
        "\n",
        "def alignment_plot_subsequent(X, y, a, Ws, sigma, label, epochs,gamma = None):\n",
        "  # You can further analyze the evolution of the leading_right_svs list\n",
        "  # For example, calculate the cosine similarity between consecutive vectors\n",
        "  similarities = []\n",
        "  X = X.cpu()\n",
        "  a = a.cpu()\n",
        "  y = y.cpu()\n",
        "  for i in range(len(Ws)-1):\n",
        "      # Ensure vectors are on CPU for numpy if needed, or use torch dot product\n",
        "      vec1 = (X.T @ (y - network_forward(X, Ws[i+1], a, sigma, gamma = 1)) ).flatten()\n",
        "      vec1 /= vec1.norm() #leading_right_svs[i]\n",
        "      vec2 = (X.T @ (y - network_forward(X, Ws[i], a, sigma, gamma = 1)) ).flatten() #\n",
        "      vec2 /= vec2.norm()\n",
        "\n",
        "      # Cosine similarity = dot(v1, v2) / (norm(v1) * norm(v2))\n",
        "      # Since norms should be 1, it's just the dot product\n",
        "      sim = torch.dot(vec1.cpu(), vec2.cpu()).abs().item()\n",
        "      similarities.append(sim)\n",
        "\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  plt.scatter(range(epochs-1), similarities)\n",
        "  plt.xlabel(\"Epoch\", fontsize = 16)\n",
        "  plt.ylabel(r\"Dot Product\", fontsize = 16)\n",
        "  plt.title(label, fontsize = 16)\n",
        "  plt.grid(True)\n",
        "  plt.xscale('log')\n",
        "  # plt.show()\n",
        "\n",
        "  plt.savefig(path+f\"Alignment-subsequent-{label}-gamma-{gamma}-lr-{lr}.png\", dpi = 100)\n",
        "\n",
        "  plt.close()"
      ],
      "metadata": {
        "id": "muRCnf-rULfm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lxxvYPHZDWJH"
      },
      "outputs": [],
      "source": [
        "def run_exp_and_plot_ntk(lr,gamma,W,a):\n",
        "\n",
        "  W_NTK = copy.deepcopy(W)\n",
        "  a_NTK = copy.deepcopy(a)\n",
        "\n",
        "  W_MF = copy.deepcopy(W)\n",
        "  a_MF = copy.deepcopy(a)\n",
        "\n",
        "  losses_NTK, operator_norms_NTK, spectrum_NTK, leading_right_svs_NTK, Ws_NTK, test_losses_NTK, energy_NTK = train_track(X,y,Xtst,ytst,W_NTK,a_NTK,sigmoid_act,1,lr,epochs)\n",
        "  losses_MF, operator_norms_MF, spectrum_MF, leading_right_svs_MF, Ws_MF, test_losses_MF, energy_MF = train_track(X,y,Xtst,ytst,W_MF,a_MF,sigmoid_act,1/np.sqrt(k),lr,epochs)\n",
        "  # plot_train_test_loss(losses_NTK, test_losses_NTK, \"NTK\")\n",
        "  # plot_norms(spectrum_NTK, operator_norms_NTK, \"NTK\")\n",
        "  # plot_train_test_loss(losses_MF, test_losses_MF, \"MF\")\n",
        "  # plot_norms(spectrum_MF, operator_norms_MF, \"MF\")\n",
        "  # alignment_plot(leading_right_svs_NTK, \"NTK Data Alignment\", epochs, q = q, gamma = 1)\n",
        "  # alignment_plot(leading_right_svs_NTK, \"NTK Residue Alignment\", epochs, Ws = Ws_NTK, a = a_NTK, act = sigmoid_act, gamma = 1)\n",
        "  # alignment_plot(leading_right_svs_MF, \"MF Data Alignment\", epochs, q = q)\n",
        "  # alignment_plot(leading_right_svs_MF, \"MF Residue Alignment\", epochs, Ws = Ws_MF, a = a_MF, act = sigmoid_act, gamma = 1)\n",
        "  # alignment_plot_subsequent(X, y, a, Ws_NTK, sigmoid_act, \"NTK Subsequent Residue Alignment\", epochs, gamma = \"NTK\")\n",
        "  # alignment_plot_subsequent(X, y, a, Ws_MF, sigmoid_act, \"MF Subsequent Residue Alignment\", epochs, gamma = \"MF\")\n",
        "  alignment_plot_initial(X, y, a, leading_right_svs_NTK, sigmoid_act, \"NTK Initial Residue Alignment\", epochs, gamma = \"NTK\")\n",
        "  alignment_plot_initial(X, y, a, leading_right_svs_MF, sigmoid_act, \"MF Initial Residue Alignment\", epochs, gamma = \"MF\")\n",
        "  angles_plot(Ws_NTK, Ws_MF, \"NTK vs MF\", gamma = \"Both\")\n",
        "  energy_plot(energy_NTK, \"NTK\")\n",
        "  energy_plot(energy_MF, \"MF\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0aZBNkjRDWJM"
      },
      "outputs": [],
      "source": [
        "nu = 0\n",
        "alpha = 0\n",
        "\n",
        "# --- Configuration ---\n",
        "seed = 42          # For reproducibility\n",
        "epochs = 50       # Number of training iterations\n",
        "d = 1000            # Input dimension\n",
        "k = 1250           # Number of hidden neurons (rows of W)\n",
        "N = 750            # Number of data points (full batch)\n",
        "\n",
        "q = torch.randn(d,1)\n",
        "q = q/torch.norm(q)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C034HNxYDWJM",
        "outputId": "ec226ad3-19e2-4c7c-9871-8fe6536f5fbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing model: k=1250, d=1000\n",
            "Model initialized.\n"
          ]
        }
      ],
      "source": [
        "# --- Model Initialization ---\n",
        "print(f\"Initializing model: k={k}, d={d}\")\n",
        "# Initialize W with rows uniformly on the unit sphere\n",
        "W = torch.randn(k, d, device=device)\n",
        "W = W/ torch.norm(W, p=2, dim=1, keepdim=True)\n",
        "W.requires_grad_(True) # We want to compute gradients for W\n",
        "\n",
        "# Initialize fixed vector 'a' with Gaussian entries (scaled)\n",
        "# Variance 1/k ensures E[||a||^2] = 1\n",
        "a = torch.randn(k, 1, device=device) / math.sqrt(k)\n",
        "a.requires_grad_(False) # 'a' is fixed\n",
        "print(\"Model initialized.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vAMjLXekDWJN"
      },
      "outputs": [],
      "source": [
        "beta = torch.randn(d,1)\n",
        "beta = beta/torch.norm(beta)\n",
        "\n",
        "def f(x):\n",
        "  return activation(x.T @ beta, act = \"sigmoid\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0h2vNa9LDWJN"
      },
      "outputs": [],
      "source": [
        "X, z, X_B, X_S = gen_data(N, d, N**nu, q, alpha, return_all=True)\n",
        "y = get_y(X, f).reshape(-1,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xamji-uMDWJN"
      },
      "outputs": [],
      "source": [
        "X = X.to('cuda')\n",
        "y = y.to('cuda')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A3jh7peyDWJN"
      },
      "outputs": [],
      "source": [
        "Xtst = gen_data(N, d, N**nu, q, alpha)\n",
        "ytst = get_y(Xtst, f)\n",
        "\n",
        "Xtst = Xtst.to('cuda')\n",
        "ytst = ytst.to('cuda')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xI2KrP-iDWJO",
        "outputId": "f8187f25-6a47-4ca6-b9e1-b3890e2e47b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training for 50 epochs with lr=17.67766952966369\n",
            "Epoch [50/50], Loss: 14.358500, Test Loss: 10.037251, Operator Norm: 14.3799\n",
            "Training finished.\n",
            "Final Test Loss: 3.362156\n",
            "Starting training for 50 epochs with lr=17.67766952966369\n",
            "Epoch [50/50], Loss: 0.925410, Test Loss: 1.269230, Operator Norm: 33.7112\n",
            "Training finished.\n",
            "Final Test Loss: 1.222997\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50/50 [00:25<00:00,  1.99it/s]\n"
          ]
        }
      ],
      "source": [
        "lr = np.sqrt(k)/2          # Fixed learning rate\n",
        "path = \"\"\n",
        "run_exp_and_plot_ntk(lr,None,W,a)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}